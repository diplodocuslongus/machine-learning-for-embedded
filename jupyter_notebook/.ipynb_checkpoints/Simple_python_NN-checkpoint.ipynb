{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NN in python\n",
    "\n",
    "This NN model is a naive implementation of a Neural Network in Python and is going to be used to train and then compare and test with a few embedded platforms. The class code is taken from [here](https://github.com/llSourcell/Make_a_neural_network/blob/master/demo.py).\n",
    "\n",
    "## Prerequisites\n",
    "You need to install jupyter notebook for this example to run. The easiest way is to install miniconda and then use conda to install numpy and jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import exp, array, random, dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        # Seed the random number generator, so it generates the same numbers\n",
    "        # every time the program runs.\n",
    "        random.seed(1)\n",
    "\n",
    "        # We model a single neuron, with 3 input connections and 1 output connection.\n",
    "        # We assign random weights to a 3 x 1 matrix, with values in the range -1 to 1\n",
    "        # and mean 0.\n",
    "        self.synaptic_weights = 2 * random.random((3, 1)) - 1\n",
    "\n",
    "    # The Sigmoid function, which describes an S shaped curve.\n",
    "    # We pass the weighted sum of the inputs through this function to\n",
    "    # normalise them between 0 and 1.\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + exp(-x))\n",
    "\n",
    "    # The derivative of the Sigmoid function.\n",
    "    # This is the gradient of the Sigmoid curve.\n",
    "    # It indicates how confident we are about the existing weight.\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    # We train the neural network through a process of trial and error.\n",
    "    # Adjusting the synaptic weights each time.\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "        for iteration in range(number_of_training_iterations):\n",
    "            # Pass the training set through our neural network (a single neuron).\n",
    "            output = self.predict(training_set_inputs)\n",
    "\n",
    "            # Calculate the error (The difference between the desired output\n",
    "            # and the predicted output).\n",
    "            error = training_set_outputs - output\n",
    "\n",
    "            # Multiply the error by the input and again by the gradient of the Sigmoid curve.\n",
    "            # This means less confident weights are adjusted more.\n",
    "            # This means inputs, which are zero, do not cause changes to the weights.\n",
    "            adjustment = dot(training_set_inputs.T, error * self.__sigmoid_derivative(output))\n",
    "\n",
    "            # Adjust the weights.\n",
    "            self.synaptic_weights += adjustment\n",
    "\n",
    "    # The neural network predicts.\n",
    "    def predict(self, inputs):\n",
    "        # Pass inputs through our neural network (our single neuron).\n",
    "        return self.__sigmoid(dot(inputs, self.synaptic_weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Intialise a single neuron neural network\n",
    "\n",
    "First we need to create an `NeuralNetwork` object and initialize it. In this case initialization means that the weights will get a random value, but with using a constant seed, so the weights are re-producable per run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random starting synaptic weights:\n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Random starting synaptic weights:\")\n",
    "print(neural_network.synaptic_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a labeled train set with data and labels.\n",
    "\n",
    "Now we need to create our input data and then label them. In this case `label` just means the output, so we have two labels 0 and 1, or you can just think of these labels as binary outputs, in this case. The data are those on the next table:\n",
    "\n",
    "D2 | D1 | D0 | Label\n",
    "-|-|-|-\n",
    "0 | 0 | 1 | 0\n",
    "1 | 1 | 1 | 1\n",
    "1 | 0 | 1 | 1\n",
    "0 | 1 | 1 | 0\n",
    "\n",
    "In Python the above table is translated to those two arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Karnaugh= [[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]]\n",
    "training_set_inputs = array(Karnaugh)\n",
    "training_set_outputs = array([[0, 1, 1, 0]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the neural network. using a training set.\n",
    "\n",
    "Now using the above training set we can train our NN. To do that we can run it 10,000 times and make small adjustments each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New synaptic weights after training: \n",
      "\n",
      "[[ 9.67299303]\n",
      " [-0.2078435 ]\n",
      " [-4.62963669]]\n"
     ]
    }
   ],
   "source": [
    "neural_network.train(training_set_inputs, training_set_outputs, 10000)\n",
    "\n",
    "print(\"New synaptic weights after training: \\n\")\n",
    "print(neural_network.synaptic_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the trained model\n",
    "\n",
    "Now that we have a trained NN, we can test it by passing input data that it doesn't know about.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All possible inputs\n",
    "inputs = array([\n",
    "    [0,0,0],\n",
    "    [0,0,1],\n",
    "    [0,1,0],\n",
    "    [0,1,1],\n",
    "    [1,0,0],\n",
    "    [1,0,1],\n",
    "    [1,1,0],\n",
    "    [1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0] = [0.5]\n",
      "[0 0 1] = [0.009664]\n",
      "[0 1 0] = [0.44822538]\n",
      "[0 1 1] = [0.00786466]\n",
      "[1 0 0] = [0.99993704]\n",
      "[1 0 1] = [0.99358931]\n",
      "[1 1 0] = [0.9999225]\n",
      "[1 1 1] = [0.99211997]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 8):\n",
    "    print(\"{} = {}\".format(inputs[i], neural_network.predict(inputs[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
